C:\Users\Daniel-PC\PycharmProjects\cnn\venv\Scripts\python.exe C:/Users/Daniel-PC/PycharmProjects/cnn/venv/r4_densenet.py
2021-05-15 14:58:52.790625: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:349: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.
  warnings.warn('This ImageDataGenerator specifies '
3
Found 5983 images belonging to 3 classes.
Found 1490 images belonging to 3 classes.
2021-05-15 14:58:55.583825: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-05-15 14:58:55.584820: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-05-15 14:58:56.264174: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2021-05-15 14:58:56.264622: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-05-15 14:58:56.275385: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-15 14:58:56.275624: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-15 14:58:56.279592: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-05-15 14:58:56.281015: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-05-15 14:58:56.289984: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-05-15 14:58:56.293883: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-05-15 14:58:56.294672: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-15 14:58:56.294962: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-05-15 14:58:56.295362: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-05-15 14:58:56.296284: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2021-05-15 14:58:56.296901: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-05-15 14:58:56.297170: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-15 14:58:56.297408: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-15 14:58:56.297660: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-05-15 14:58:56.297893: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-05-15 14:58:56.298133: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-05-15 14:58:56.298336: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-05-15 14:58:56.298538: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-15 14:58:56.298767: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-05-15 14:58:56.839726: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-05-15 14:58:56.839965: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-05-15 14:58:56.840105: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-05-15 14:58:56.840396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce MX250, pci bus id: 0000:02:00.0, compute capability: 6.1)
2021-05-15 14:58:56.841346: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
input_1
zero_padding2d_2
conv1/conv
conv1/bn
conv1/relu
zero_padding2d_3
pool1
conv2_block1_0_bn
conv2_block1_0_relu
conv2_block1_1_conv
conv2_block1_1_bn
conv2_block1_1_relu
conv2_block1_2_conv
conv2_block1_concat
conv2_block2_0_bn
conv2_block2_0_relu
conv2_block2_1_conv
conv2_block2_1_bn
conv2_block2_1_relu
conv2_block2_2_conv
conv2_block2_concat
conv2_block3_0_bn
conv2_block3_0_relu
conv2_block3_1_conv
conv2_block3_1_bn
conv2_block3_1_relu
conv2_block3_2_conv
conv2_block3_concat
conv2_block4_0_bn
conv2_block4_0_relu
conv2_block4_1_conv
conv2_block4_1_bn
conv2_block4_1_relu
conv2_block4_2_conv
conv2_block4_concat
conv2_block5_0_bn
conv2_block5_0_relu
conv2_block5_1_conv
conv2_block5_1_bn
conv2_block5_1_relu
conv2_block5_2_conv
conv2_block5_concat
conv2_block6_0_bn
conv2_block6_0_relu
conv2_block6_1_conv
conv2_block6_1_bn
conv2_block6_1_relu
conv2_block6_2_conv
conv2_block6_concat
pool2_bn
pool2_relu
pool2_conv
pool2_pool
conv3_block1_0_bn
conv3_block1_0_relu
conv3_block1_1_conv
conv3_block1_1_bn
conv3_block1_1_relu
conv3_block1_2_conv
conv3_block1_concat
conv3_block2_0_bn
conv3_block2_0_relu
conv3_block2_1_conv
conv3_block2_1_bn
conv3_block2_1_relu
conv3_block2_2_conv
conv3_block2_concat
conv3_block3_0_bn
conv3_block3_0_relu
conv3_block3_1_conv
conv3_block3_1_bn
conv3_block3_1_relu
conv3_block3_2_conv
conv3_block3_concat
conv3_block4_0_bn
conv3_block4_0_relu
conv3_block4_1_conv
conv3_block4_1_bn
conv3_block4_1_relu
conv3_block4_2_conv
conv3_block4_concat
conv3_block5_0_bn
conv3_block5_0_relu
conv3_block5_1_conv
conv3_block5_1_bn
conv3_block5_1_relu
conv3_block5_2_conv
conv3_block5_concat
conv3_block6_0_bn
conv3_block6_0_relu
conv3_block6_1_conv
conv3_block6_1_bn
conv3_block6_1_relu
conv3_block6_2_conv
conv3_block6_concat
conv3_block7_0_bn
conv3_block7_0_relu
conv3_block7_1_conv
conv3_block7_1_bn
conv3_block7_1_relu
conv3_block7_2_conv
conv3_block7_concat
conv3_block8_0_bn
conv3_block8_0_relu
conv3_block8_1_conv
conv3_block8_1_bn
conv3_block8_1_relu
conv3_block8_2_conv
conv3_block8_concat
conv3_block9_0_bn
conv3_block9_0_relu
conv3_block9_1_conv
conv3_block9_1_bn
conv3_block9_1_relu
conv3_block9_2_conv
conv3_block9_concat
conv3_block10_0_bn
conv3_block10_0_relu
conv3_block10_1_conv
conv3_block10_1_bn
conv3_block10_1_relu
conv3_block10_2_conv
conv3_block10_concat
conv3_block11_0_bn
conv3_block11_0_relu
conv3_block11_1_conv
conv3_block11_1_bn
conv3_block11_1_relu
conv3_block11_2_conv
conv3_block11_concat
conv3_block12_0_bn
conv3_block12_0_relu
conv3_block12_1_conv
conv3_block12_1_bn
conv3_block12_1_relu
conv3_block12_2_conv
conv3_block12_concat
pool3_bn
pool3_relu
pool3_conv
pool3_pool
conv4_block1_0_bn
conv4_block1_0_relu
conv4_block1_1_conv
conv4_block1_1_bn
conv4_block1_1_relu
conv4_block1_2_conv
conv4_block1_concat
conv4_block2_0_bn
conv4_block2_0_relu
conv4_block2_1_conv
conv4_block2_1_bn
conv4_block2_1_relu
conv4_block2_2_conv
conv4_block2_concat
conv4_block3_0_bn
conv4_block3_0_relu
conv4_block3_1_conv
conv4_block3_1_bn
conv4_block3_1_relu
conv4_block3_2_conv
conv4_block3_concat
conv4_block4_0_bn
conv4_block4_0_relu
conv4_block4_1_conv
conv4_block4_1_bn
conv4_block4_1_relu
conv4_block4_2_conv
conv4_block4_concat
conv4_block5_0_bn
conv4_block5_0_relu
conv4_block5_1_conv
conv4_block5_1_bn
conv4_block5_1_relu
conv4_block5_2_conv
conv4_block5_concat
conv4_block6_0_bn
conv4_block6_0_relu
conv4_block6_1_conv
conv4_block6_1_bn
conv4_block6_1_relu
conv4_block6_2_conv
conv4_block6_concat
conv4_block7_0_bn
conv4_block7_0_relu
conv4_block7_1_conv
conv4_block7_1_bn
conv4_block7_1_relu
conv4_block7_2_conv
conv4_block7_concat
conv4_block8_0_bn
conv4_block8_0_relu
conv4_block8_1_conv
conv4_block8_1_bn
conv4_block8_1_relu
conv4_block8_2_conv
conv4_block8_concat
conv4_block9_0_bn
conv4_block9_0_relu
conv4_block9_1_conv
conv4_block9_1_bn
conv4_block9_1_relu
conv4_block9_2_conv
conv4_block9_concat
conv4_block10_0_bn
conv4_block10_0_relu
conv4_block10_1_conv
conv4_block10_1_bn
conv4_block10_1_relu
conv4_block10_2_conv
conv4_block10_concat
conv4_block11_0_bn
conv4_block11_0_relu
conv4_block11_1_conv
conv4_block11_1_bn
conv4_block11_1_relu
conv4_block11_2_conv
conv4_block11_concat
conv4_block12_0_bn
conv4_block12_0_relu
conv4_block12_1_conv
conv4_block12_1_bn
conv4_block12_1_relu
conv4_block12_2_conv
conv4_block12_concat
conv4_block13_0_bn
conv4_block13_0_relu
conv4_block13_1_conv
conv4_block13_1_bn
conv4_block13_1_relu
conv4_block13_2_conv
conv4_block13_concat
conv4_block14_0_bn
conv4_block14_0_relu
conv4_block14_1_conv
conv4_block14_1_bn
conv4_block14_1_relu
conv4_block14_2_conv
conv4_block14_concat
conv4_block15_0_bn
conv4_block15_0_relu
conv4_block15_1_conv
conv4_block15_1_bn
conv4_block15_1_relu
conv4_block15_2_conv
conv4_block15_concat
conv4_block16_0_bn
conv4_block16_0_relu
conv4_block16_1_conv
conv4_block16_1_bn
conv4_block16_1_relu
conv4_block16_2_conv
conv4_block16_concat
conv4_block17_0_bn
conv4_block17_0_relu
conv4_block17_1_conv
conv4_block17_1_bn
conv4_block17_1_relu
conv4_block17_2_conv
conv4_block17_concat
conv4_block18_0_bn
conv4_block18_0_relu
conv4_block18_1_conv
conv4_block18_1_bn
conv4_block18_1_relu
conv4_block18_2_conv
conv4_block18_concat
conv4_block19_0_bn
conv4_block19_0_relu
conv4_block19_1_conv
conv4_block19_1_bn
conv4_block19_1_relu
conv4_block19_2_conv
conv4_block19_concat
conv4_block20_0_bn
conv4_block20_0_relu
conv4_block20_1_conv
conv4_block20_1_bn
conv4_block20_1_relu
conv4_block20_2_conv
conv4_block20_concat
conv4_block21_0_bn
conv4_block21_0_relu
conv4_block21_1_conv
conv4_block21_1_bn
conv4_block21_1_relu
conv4_block21_2_conv
conv4_block21_concat
conv4_block22_0_bn
conv4_block22_0_relu
conv4_block22_1_conv
conv4_block22_1_bn
conv4_block22_1_relu
conv4_block22_2_conv
conv4_block22_concat
conv4_block23_0_bn
conv4_block23_0_relu
conv4_block23_1_conv
conv4_block23_1_bn
conv4_block23_1_relu
conv4_block23_2_conv
conv4_block23_concat
conv4_block24_0_bn
conv4_block24_0_relu
conv4_block24_1_conv
conv4_block24_1_bn
conv4_block24_1_relu
conv4_block24_2_conv
conv4_block24_concat
pool4_bn
pool4_relu
pool4_conv
pool4_pool
conv5_block1_0_bn
conv5_block1_0_relu
conv5_block1_1_conv
conv5_block1_1_bn
conv5_block1_1_relu
conv5_block1_2_conv
conv5_block1_concat
conv5_block2_0_bn
conv5_block2_0_relu
conv5_block2_1_conv
conv5_block2_1_bn
conv5_block2_1_relu
conv5_block2_2_conv
conv5_block2_concat
conv5_block3_0_bn
conv5_block3_0_relu
conv5_block3_1_conv
conv5_block3_1_bn
conv5_block3_1_relu
conv5_block3_2_conv
conv5_block3_concat
conv5_block4_0_bn
conv5_block4_0_relu
conv5_block4_1_conv
conv5_block4_1_bn
conv5_block4_1_relu
conv5_block4_2_conv
conv5_block4_concat
conv5_block5_0_bn
conv5_block5_0_relu
conv5_block5_1_conv
conv5_block5_1_bn
conv5_block5_1_relu
conv5_block5_2_conv
conv5_block5_concat
conv5_block6_0_bn
conv5_block6_0_relu
conv5_block6_1_conv
conv5_block6_1_bn
conv5_block6_1_relu
conv5_block6_2_conv
conv5_block6_concat
conv5_block7_0_bn
conv5_block7_0_relu
conv5_block7_1_conv
conv5_block7_1_bn
conv5_block7_1_relu
conv5_block7_2_conv
conv5_block7_concat
conv5_block8_0_bn
conv5_block8_0_relu
conv5_block8_1_conv
conv5_block8_1_bn
conv5_block8_1_relu
conv5_block8_2_conv
conv5_block8_concat
conv5_block9_0_bn
conv5_block9_0_relu
conv5_block9_1_conv
conv5_block9_1_bn
conv5_block9_1_relu
conv5_block9_2_conv
conv5_block9_concat
conv5_block10_0_bn
conv5_block10_0_relu
conv5_block10_1_conv
conv5_block10_1_bn
conv5_block10_1_relu
conv5_block10_2_conv
conv5_block10_concat
conv5_block11_0_bn
conv5_block11_0_relu
conv5_block11_1_conv
conv5_block11_1_bn
conv5_block11_1_relu
conv5_block11_2_conv
conv5_block11_concat
conv5_block12_0_bn
conv5_block12_0_relu
conv5_block12_1_conv
conv5_block12_1_bn
conv5_block12_1_relu
conv5_block12_2_conv
conv5_block12_concat
conv5_block13_0_bn
conv5_block13_0_relu
conv5_block13_1_conv
conv5_block13_1_bn
conv5_block13_1_relu
conv5_block13_2_conv
conv5_block13_concat
conv5_block14_0_bn
conv5_block14_0_relu
conv5_block14_1_conv
conv5_block14_1_bn
conv5_block14_1_relu
conv5_block14_2_conv
conv5_block14_concat
conv5_block15_0_bn
conv5_block15_0_relu
conv5_block15_1_conv
conv5_block15_1_bn
conv5_block15_1_relu
conv5_block15_2_conv
conv5_block15_concat
conv5_block16_0_bn
conv5_block16_0_relu
conv5_block16_1_conv
conv5_block16_1_bn
conv5_block16_1_relu
conv5_block16_2_conv
conv5_block16_concat
bn
relu
input_1_mirror
zero_padding2d_6
conv1/conv
conv1/bn
conv1/relu
zero_padding2d_7
pool1
conv2_block1_0_bn
conv2_block1_0_relu
conv2_block1_1_conv
conv2_block1_1_bn
conv2_block1_1_relu
conv2_block1_2_conv
conv2_block1_concat
conv2_block2_0_bn
conv2_block2_0_relu
conv2_block2_1_conv
conv2_block2_1_bn
conv2_block2_1_relu
conv2_block2_2_conv
conv2_block2_concat
conv2_block3_0_bn
conv2_block3_0_relu
conv2_block3_1_conv
conv2_block3_1_bn
conv2_block3_1_relu
conv2_block3_2_conv
conv2_block3_concat
conv2_block4_0_bn
conv2_block4_0_relu
conv2_block4_1_conv
conv2_block4_1_bn
conv2_block4_1_relu
conv2_block4_2_conv
conv2_block4_concat
conv2_block5_0_bn
conv2_block5_0_relu
conv2_block5_1_conv
conv2_block5_1_bn
conv2_block5_1_relu
conv2_block5_2_conv
conv2_block5_concat
conv2_block6_0_bn
conv2_block6_0_relu
conv2_block6_1_conv
conv2_block6_1_bn
conv2_block6_1_relu
conv2_block6_2_conv
conv2_block6_concat
pool2_bn
pool2_relu
pool2_conv
pool2_pool
conv3_block1_0_bn
conv3_block1_0_relu
conv3_block1_1_conv
conv3_block1_1_bn
conv3_block1_1_relu
conv3_block1_2_conv
conv3_block1_concat
conv3_block2_0_bn
conv3_block2_0_relu
conv3_block2_1_conv
conv3_block2_1_bn
conv3_block2_1_relu
conv3_block2_2_conv
conv3_block2_concat
conv3_block3_0_bn
conv3_block3_0_relu
conv3_block3_1_conv
conv3_block3_1_bn
conv3_block3_1_relu
conv3_block3_2_conv
conv3_block3_concat
conv3_block4_0_bn
conv3_block4_0_relu
conv3_block4_1_conv
conv3_block4_1_bn
conv3_block4_1_relu
conv3_block4_2_conv
conv3_block4_concat
conv3_block5_0_bn
conv3_block5_0_relu
conv3_block5_1_conv
conv3_block5_1_bn
conv3_block5_1_relu
conv3_block5_2_conv
conv3_block5_concat
conv3_block6_0_bn
conv3_block6_0_relu
conv3_block6_1_conv
conv3_block6_1_bn
conv3_block6_1_relu
conv3_block6_2_conv
conv3_block6_concat
conv3_block7_0_bn
conv3_block7_0_relu
conv3_block7_1_conv
conv3_block7_1_bn
conv3_block7_1_relu
conv3_block7_2_conv
conv3_block7_concat
conv3_block8_0_bn
conv3_block8_0_relu
conv3_block8_1_conv
conv3_block8_1_bn
conv3_block8_1_relu
conv3_block8_2_conv
conv3_block8_concat
conv3_block9_0_bn
conv3_block9_0_relu
conv3_block9_1_conv
conv3_block9_1_bn
conv3_block9_1_relu
conv3_block9_2_conv
conv3_block9_concat
conv3_block10_0_bn
conv3_block10_0_relu
conv3_block10_1_conv
conv3_block10_1_bn
conv3_block10_1_relu
conv3_block10_2_conv
conv3_block10_concat
conv3_block11_0_bn
conv3_block11_0_relu
conv3_block11_1_conv
conv3_block11_1_bn
conv3_block11_1_relu
conv3_block11_2_conv
conv3_block11_concat
conv3_block12_0_bn
conv3_block12_0_relu
conv3_block12_1_conv
conv3_block12_1_bn
conv3_block12_1_relu
conv3_block12_2_conv
conv3_block12_concat
pool3_bn
pool3_relu
pool3_conv
pool3_pool
conv4_block1_0_bn
conv4_block1_0_relu
conv4_block1_1_conv
conv4_block1_1_bn
conv4_block1_1_relu
conv4_block1_2_conv
conv4_block1_concat
conv4_block2_0_bn
conv4_block2_0_relu
conv4_block2_1_conv
conv4_block2_1_bn
conv4_block2_1_relu
conv4_block2_2_conv
conv4_block2_concat
conv4_block3_0_bn
conv4_block3_0_relu
conv4_block3_1_conv
conv4_block3_1_bn
conv4_block3_1_relu
conv4_block3_2_conv
conv4_block3_concat
conv4_block4_0_bn
conv4_block4_0_relu
conv4_block4_1_conv
conv4_block4_1_bn
conv4_block4_1_relu
conv4_block4_2_conv
conv4_block4_concat
conv4_block5_0_bn
conv4_block5_0_relu
conv4_block5_1_conv
conv4_block5_1_bn
conv4_block5_1_relu
conv4_block5_2_conv
conv4_block5_concat
conv4_block6_0_bn
conv4_block6_0_relu
conv4_block6_1_conv
conv4_block6_1_bn
conv4_block6_1_relu
conv4_block6_2_conv
conv4_block6_concat
conv4_block7_0_bn
conv4_block7_0_relu
conv4_block7_1_conv
conv4_block7_1_bn
conv4_block7_1_relu
conv4_block7_2_conv
conv4_block7_concat
conv4_block8_0_bn
conv4_block8_0_relu
conv4_block8_1_conv
conv4_block8_1_bn
conv4_block8_1_relu
conv4_block8_2_conv
conv4_block8_concat
conv4_block9_0_bn
conv4_block9_0_relu
conv4_block9_1_conv
conv4_block9_1_bn
conv4_block9_1_relu
conv4_block9_2_conv
conv4_block9_concat
conv4_block10_0_bn
conv4_block10_0_relu
conv4_block10_1_conv
conv4_block10_1_bn
conv4_block10_1_relu
conv4_block10_2_conv
conv4_block10_concat
conv4_block11_0_bn
conv4_block11_0_relu
conv4_block11_1_conv
conv4_block11_1_bn
conv4_block11_1_relu
conv4_block11_2_conv
conv4_block11_concat
conv4_block12_0_bn
conv4_block12_0_relu
conv4_block12_1_conv
conv4_block12_1_bn
conv4_block12_1_relu
conv4_block12_2_conv
conv4_block12_concat
conv4_block13_0_bn
conv4_block13_0_relu
conv4_block13_1_conv
conv4_block13_1_bn
conv4_block13_1_relu
conv4_block13_2_conv
conv4_block13_concat
conv4_block14_0_bn
conv4_block14_0_relu
conv4_block14_1_conv
conv4_block14_1_bn
conv4_block14_1_relu
conv4_block14_2_conv
conv4_block14_concat
conv4_block15_0_bn
conv4_block15_0_relu
conv4_block15_1_conv
conv4_block15_1_bn
conv4_block15_1_relu
conv4_block15_2_conv
conv4_block15_concat
conv4_block16_0_bn
conv4_block16_0_relu
conv4_block16_1_conv
conv4_block16_1_bn
conv4_block16_1_relu
conv4_block16_2_conv
conv4_block16_concat
conv4_block17_0_bn
conv4_block17_0_relu
conv4_block17_1_conv
conv4_block17_1_bn
conv4_block17_1_relu
conv4_block17_2_conv
conv4_block17_concat
conv4_block18_0_bn
conv4_block18_0_relu
conv4_block18_1_conv
conv4_block18_1_bn
conv4_block18_1_relu
conv4_block18_2_conv
conv4_block18_concat
conv4_block19_0_bn
conv4_block19_0_relu
conv4_block19_1_conv
conv4_block19_1_bn
conv4_block19_1_relu
conv4_block19_2_conv
conv4_block19_concat
conv4_block20_0_bn
conv4_block20_0_relu
conv4_block20_1_conv
conv4_block20_1_bn
conv4_block20_1_relu
conv4_block20_2_conv
conv4_block20_concat
conv4_block21_0_bn
conv4_block21_0_relu
conv4_block21_1_conv
conv4_block21_1_bn
conv4_block21_1_relu
conv4_block21_2_conv
conv4_block21_concat
conv4_block22_0_bn
conv4_block22_0_relu
conv4_block22_1_conv
conv4_block22_1_bn
conv4_block22_1_relu
conv4_block22_2_conv
conv4_block22_concat
conv4_block23_0_bn
conv4_block23_0_relu
conv4_block23_1_conv
conv4_block23_1_bn
conv4_block23_1_relu
conv4_block23_2_conv
conv4_block23_concat
conv4_block24_0_bn
conv4_block24_0_relu
conv4_block24_1_conv
conv4_block24_1_bn
conv4_block24_1_relu
conv4_block24_2_conv
conv4_block24_concat
pool4_bn
pool4_relu
pool4_conv
pool4_pool
conv5_block1_0_bn
conv5_block1_0_relu
conv5_block1_1_conv
conv5_block1_1_bn
conv5_block1_1_relu
conv5_block1_2_conv
conv5_block1_concat
conv5_block2_0_bn
conv5_block2_0_relu
conv5_block2_1_conv
conv5_block2_1_bn
conv5_block2_1_relu
conv5_block2_2_conv
conv5_block2_concat
conv5_block3_0_bn
conv5_block3_0_relu
conv5_block3_1_conv
conv5_block3_1_bn
conv5_block3_1_relu
conv5_block3_2_conv
conv5_block3_concat
conv5_block4_0_bn
conv5_block4_0_relu
conv5_block4_1_conv
conv5_block4_1_bn
conv5_block4_1_relu
conv5_block4_2_conv
conv5_block4_concat
conv5_block5_0_bn
conv5_block5_0_relu
conv5_block5_1_conv
conv5_block5_1_bn
conv5_block5_1_relu
conv5_block5_2_conv
conv5_block5_concat
conv5_block6_0_bn
conv5_block6_0_relu
conv5_block6_1_conv
conv5_block6_1_bn
conv5_block6_1_relu
conv5_block6_2_conv
conv5_block6_concat
conv5_block7_0_bn
conv5_block7_0_relu
conv5_block7_1_conv
conv5_block7_1_bn
conv5_block7_1_relu
conv5_block7_2_conv
conv5_block7_concat
conv5_block8_0_bn
conv5_block8_0_relu
conv5_block8_1_conv
conv5_block8_1_bn
conv5_block8_1_relu
conv5_block8_2_conv
conv5_block8_concat
conv5_block9_0_bn
conv5_block9_0_relu
conv5_block9_1_conv
conv5_block9_1_bn
conv5_block9_1_relu
conv5_block9_2_conv
conv5_block9_concat
conv5_block10_0_bn
conv5_block10_0_relu
conv5_block10_1_conv
conv5_block10_1_bn
conv5_block10_1_relu
conv5_block10_2_conv
conv5_block10_concat
conv5_block11_0_bn
conv5_block11_0_relu
conv5_block11_1_conv
conv5_block11_1_bn
conv5_block11_1_relu
conv5_block11_2_conv
conv5_block11_concat
conv5_block12_0_bn
conv5_block12_0_relu
conv5_block12_1_conv
conv5_block12_1_bn
conv5_block12_1_relu
conv5_block12_2_conv
conv5_block12_concat
conv5_block13_0_bn
conv5_block13_0_relu
conv5_block13_1_conv
conv5_block13_1_bn
conv5_block13_1_relu
conv5_block13_2_conv
conv5_block13_concat
conv5_block14_0_bn
conv5_block14_0_relu
conv5_block14_1_conv
conv5_block14_1_bn
conv5_block14_1_relu
conv5_block14_2_conv
conv5_block14_concat
conv5_block15_0_bn
conv5_block15_0_relu
conv5_block15_1_conv
conv5_block15_1_bn
conv5_block15_1_relu
conv5_block15_2_conv
conv5_block15_concat
conv5_block16_0_bn
conv5_block16_0_relu
conv5_block16_1_conv
conv5_block16_1_bn
conv5_block16_1_relu
conv5_block16_2_conv
conv5_block16_concat
bn
relu
Concatenation success!
Fused-DenseNet-Tiny ready to connect with its ending layers!


PLEASE CHECK THE MODEL UP TO THE END
Fused-DenseNet-Tiny complete and ready for compilation and training!



C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\keras\engine\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
  warnings.warn('`Model.fit_generator` is deprecated and '
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.
  warnings.warn('This ImageDataGenerator specifies '
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.
  warnings.warn('This ImageDataGenerator specifies '
2021-05-15 14:59:04.335587: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/12
2021-05-15 14:59:06.350271: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-15 14:59:06.648763: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-15 14:59:06.789423: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-15 14:59:07.445050: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0

2021-05-15 14:59:07.494364: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0

2021-05-15 14:59:07.980217: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 997.86MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-15 14:59:08.020303: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-15 14:59:08.455825: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-15 14:59:08.480456: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-15 14:59:08.609534: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 952.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-15 14:59:08.633060: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 973.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-15 14:59:08.672569: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 820.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-15 14:59:08.694426: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 841.09MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
387/747 [==============>...............] - ETA: 1:35 - loss: 0.8558 - accuracy: 0.61572021-05-15 15:00:52.145404: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 851.41MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-15 15:00:52.201931: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 990.64MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
747/747 [==============================] - 206s 270ms/step - loss: 0.7229 - accuracy: 0.6805
Epoch 2/12
747/747 [==============================] - 415s 556ms/step - loss: 0.3142 - accuracy: 0.8796
Epoch 3/12
747/747 [==============================] - 575s 769ms/step - loss: 0.2529 - accuracy: 0.9089
Epoch 4/12
747/747 [==============================] - 463s 620ms/step - loss: 0.2319 - accuracy: 0.9157
Epoch 5/12
747/747 [==============================] - 260s 348ms/step - loss: 0.2021 - accuracy: 0.9232
Epoch 6/12
747/747 [==============================] - 211s 282ms/step - loss: 0.2051 - accuracy: 0.9299
Epoch 7/12
747/747 [==============================] - 207s 277ms/step - loss: 0.1889 - accuracy: 0.9332
Epoch 8/12
747/747 [==============================] - 209s 279ms/step - loss: 0.1812 - accuracy: 0.9402
Epoch 9/12
747/747 [==============================] - 207s 277ms/step - loss: 0.1669 - accuracy: 0.9372
Epoch 10/12
747/747 [==============================] - 208s 278ms/step - loss: 0.1677 - accuracy: 0.9384
Epoch 11/12
747/747 [==============================] - 208s 278ms/step - loss: 0.1478 - accuracy: 0.9474
Epoch 12/12
747/747 [==============================] - 207s 277ms/step - loss: 0.1501 - accuracy: 0.9457
------------------------------------------------------------------------
Training for fold3
Found 1490 images belonging to 3 classes.
1490/1490 [==============================] - 36s 23ms/step - loss: 0.0761 - accuracy: 0.9718
Test loss: 0.07609429955482483
Test accuracy: 0.9718120694160461
              precision    recall  f1-score   support

           0       0.94      0.99      0.97       320
           1       0.95      0.97      0.96       315
           2       0.99      0.96      0.98       855

    accuracy                           0.97      1490
   macro avg       0.96      0.98      0.97      1490
weighted avg       0.97      0.97      0.97      1490

------------------------------------------------------------------------
------------------------------------------------------------------------
Score per fold
------------------------------------------------------------------------
> Fold 1 - Loss: 0.07609429955482483 - Accuracy: 0.9718120694160461%
------------------------------------------------------------------------
Average scores for all folds:
> Accuracy: 0.9718120694160461 (+- 0.0)
> Loss: 0.07609429955482483
------------------------------------------------------------------------

Process finished with exit code 0
