C:\Users\Daniel-PC\PycharmProjects\cnn\venv\Scripts\python.exe C:/Users/Daniel-PC/PycharmProjects/cnn/venv/r4_densenet.py
2021-05-17 11:40:46.390333: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
3
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:349: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.
  warnings.warn('This ImageDataGenerator specifies '
Found 5981 images belonging to 3 classes.
Found 1493 images belonging to 3 classes.
2021-05-17 11:40:59.246617: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-05-17 11:40:59.256005: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-05-17 11:40:59.738604: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2021-05-17 11:40:59.740021: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-05-17 11:40:59.827825: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-17 11:40:59.828302: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-17 11:40:59.867149: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-05-17 11:40:59.879137: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-05-17 11:41:00.012181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-05-17 11:41:00.060839: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-05-17 11:41:00.065974: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-17 11:41:00.344990: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-05-17 11:41:00.347874: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-05-17 11:41:00.352050: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2021-05-17 11:41:00.353409: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-05-17 11:41:00.354181: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-17 11:41:00.354913: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-17 11:41:00.355614: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-05-17 11:41:00.356349: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-05-17 11:41:00.357104: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-05-17 11:41:00.357825: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-05-17 11:41:00.358572: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-17 11:41:00.359474: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-05-17 11:41:02.734681: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-05-17 11:41:02.735086: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-05-17 11:41:02.735330: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-05-17 11:41:02.736701: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce MX250, pci bus id: 0000:02:00.0, compute capability: 6.1)
2021-05-17 11:41:02.739523: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
input_1
zero_padding2d_2
conv1/conv
conv1/bn
conv1/relu
zero_padding2d_3
pool1
conv2_block1_0_bn
conv2_block1_0_relu
conv2_block1_1_conv
conv2_block1_1_bn
conv2_block1_1_relu
conv2_block1_2_conv
conv2_block1_concat
conv2_block2_0_bn
conv2_block2_0_relu
conv2_block2_1_conv
conv2_block2_1_bn
conv2_block2_1_relu
conv2_block2_2_conv
conv2_block2_concat
conv2_block3_0_bn
conv2_block3_0_relu
conv2_block3_1_conv
conv2_block3_1_bn
conv2_block3_1_relu
conv2_block3_2_conv
conv2_block3_concat
conv2_block4_0_bn
conv2_block4_0_relu
conv2_block4_1_conv
conv2_block4_1_bn
conv2_block4_1_relu
conv2_block4_2_conv
conv2_block4_concat
conv2_block5_0_bn
conv2_block5_0_relu
conv2_block5_1_conv
conv2_block5_1_bn
conv2_block5_1_relu
conv2_block5_2_conv
conv2_block5_concat
conv2_block6_0_bn
conv2_block6_0_relu
conv2_block6_1_conv
conv2_block6_1_bn
conv2_block6_1_relu
conv2_block6_2_conv
conv2_block6_concat
pool2_bn
pool2_relu
pool2_conv
pool2_pool
conv3_block1_0_bn
conv3_block1_0_relu
conv3_block1_1_conv
conv3_block1_1_bn
conv3_block1_1_relu
conv3_block1_2_conv
conv3_block1_concat
conv3_block2_0_bn
conv3_block2_0_relu
conv3_block2_1_conv
conv3_block2_1_bn
conv3_block2_1_relu
conv3_block2_2_conv
conv3_block2_concat
conv3_block3_0_bn
conv3_block3_0_relu
conv3_block3_1_conv
conv3_block3_1_bn
conv3_block3_1_relu
conv3_block3_2_conv
conv3_block3_concat
conv3_block4_0_bn
conv3_block4_0_relu
conv3_block4_1_conv
conv3_block4_1_bn
conv3_block4_1_relu
conv3_block4_2_conv
conv3_block4_concat
conv3_block5_0_bn
conv3_block5_0_relu
conv3_block5_1_conv
conv3_block5_1_bn
conv3_block5_1_relu
conv3_block5_2_conv
conv3_block5_concat
conv3_block6_0_bn
conv3_block6_0_relu
conv3_block6_1_conv
conv3_block6_1_bn
conv3_block6_1_relu
conv3_block6_2_conv
conv3_block6_concat
conv3_block7_0_bn
conv3_block7_0_relu
conv3_block7_1_conv
conv3_block7_1_bn
conv3_block7_1_relu
conv3_block7_2_conv
conv3_block7_concat
conv3_block8_0_bn
conv3_block8_0_relu
conv3_block8_1_conv
conv3_block8_1_bn
conv3_block8_1_relu
conv3_block8_2_conv
conv3_block8_concat
conv3_block9_0_bn
conv3_block9_0_relu
conv3_block9_1_conv
conv3_block9_1_bn
conv3_block9_1_relu
conv3_block9_2_conv
conv3_block9_concat
conv3_block10_0_bn
conv3_block10_0_relu
conv3_block10_1_conv
conv3_block10_1_bn
conv3_block10_1_relu
conv3_block10_2_conv
conv3_block10_concat
conv3_block11_0_bn
conv3_block11_0_relu
conv3_block11_1_conv
conv3_block11_1_bn
conv3_block11_1_relu
conv3_block11_2_conv
conv3_block11_concat
conv3_block12_0_bn
conv3_block12_0_relu
conv3_block12_1_conv
conv3_block12_1_bn
conv3_block12_1_relu
conv3_block12_2_conv
conv3_block12_concat
pool3_bn
pool3_relu
pool3_conv
pool3_pool
conv4_block1_0_bn
conv4_block1_0_relu
conv4_block1_1_conv
conv4_block1_1_bn
conv4_block1_1_relu
conv4_block1_2_conv
conv4_block1_concat
conv4_block2_0_bn
conv4_block2_0_relu
conv4_block2_1_conv
conv4_block2_1_bn
conv4_block2_1_relu
conv4_block2_2_conv
conv4_block2_concat
conv4_block3_0_bn
conv4_block3_0_relu
conv4_block3_1_conv
conv4_block3_1_bn
conv4_block3_1_relu
conv4_block3_2_conv
conv4_block3_concat
conv4_block4_0_bn
conv4_block4_0_relu
conv4_block4_1_conv
conv4_block4_1_bn
conv4_block4_1_relu
conv4_block4_2_conv
conv4_block4_concat
conv4_block5_0_bn
conv4_block5_0_relu
conv4_block5_1_conv
conv4_block5_1_bn
conv4_block5_1_relu
conv4_block5_2_conv
conv4_block5_concat
conv4_block6_0_bn
conv4_block6_0_relu
conv4_block6_1_conv
conv4_block6_1_bn
conv4_block6_1_relu
conv4_block6_2_conv
conv4_block6_concat
conv4_block7_0_bn
conv4_block7_0_relu
conv4_block7_1_conv
conv4_block7_1_bn
conv4_block7_1_relu
conv4_block7_2_conv
conv4_block7_concat
conv4_block8_0_bn
conv4_block8_0_relu
conv4_block8_1_conv
conv4_block8_1_bn
conv4_block8_1_relu
conv4_block8_2_conv
conv4_block8_concat
conv4_block9_0_bn
conv4_block9_0_relu
conv4_block9_1_conv
conv4_block9_1_bn
conv4_block9_1_relu
conv4_block9_2_conv
conv4_block9_concat
conv4_block10_0_bn
conv4_block10_0_relu
conv4_block10_1_conv
conv4_block10_1_bn
conv4_block10_1_relu
conv4_block10_2_conv
conv4_block10_concat
conv4_block11_0_bn
conv4_block11_0_relu
conv4_block11_1_conv
conv4_block11_1_bn
conv4_block11_1_relu
conv4_block11_2_conv
conv4_block11_concat
conv4_block12_0_bn
conv4_block12_0_relu
conv4_block12_1_conv
conv4_block12_1_bn
conv4_block12_1_relu
conv4_block12_2_conv
conv4_block12_concat
conv4_block13_0_bn
conv4_block13_0_relu
conv4_block13_1_conv
conv4_block13_1_bn
conv4_block13_1_relu
conv4_block13_2_conv
conv4_block13_concat
conv4_block14_0_bn
conv4_block14_0_relu
conv4_block14_1_conv
conv4_block14_1_bn
conv4_block14_1_relu
conv4_block14_2_conv
conv4_block14_concat
conv4_block15_0_bn
conv4_block15_0_relu
conv4_block15_1_conv
conv4_block15_1_bn
conv4_block15_1_relu
conv4_block15_2_conv
conv4_block15_concat
conv4_block16_0_bn
conv4_block16_0_relu
conv4_block16_1_conv
conv4_block16_1_bn
conv4_block16_1_relu
conv4_block16_2_conv
conv4_block16_concat
conv4_block17_0_bn
conv4_block17_0_relu
conv4_block17_1_conv
conv4_block17_1_bn
conv4_block17_1_relu
conv4_block17_2_conv
conv4_block17_concat
conv4_block18_0_bn
conv4_block18_0_relu
conv4_block18_1_conv
conv4_block18_1_bn
conv4_block18_1_relu
conv4_block18_2_conv
conv4_block18_concat
conv4_block19_0_bn
conv4_block19_0_relu
conv4_block19_1_conv
conv4_block19_1_bn
conv4_block19_1_relu
conv4_block19_2_conv
conv4_block19_concat
conv4_block20_0_bn
conv4_block20_0_relu
conv4_block20_1_conv
conv4_block20_1_bn
conv4_block20_1_relu
conv4_block20_2_conv
conv4_block20_concat
conv4_block21_0_bn
conv4_block21_0_relu
conv4_block21_1_conv
conv4_block21_1_bn
conv4_block21_1_relu
conv4_block21_2_conv
conv4_block21_concat
conv4_block22_0_bn
conv4_block22_0_relu
conv4_block22_1_conv
conv4_block22_1_bn
conv4_block22_1_relu
conv4_block22_2_conv
conv4_block22_concat
conv4_block23_0_bn
conv4_block23_0_relu
conv4_block23_1_conv
conv4_block23_1_bn
conv4_block23_1_relu
conv4_block23_2_conv
conv4_block23_concat
conv4_block24_0_bn
conv4_block24_0_relu
conv4_block24_1_conv
conv4_block24_1_bn
conv4_block24_1_relu
conv4_block24_2_conv
conv4_block24_concat
pool4_bn
pool4_relu
pool4_conv
pool4_pool
conv5_block1_0_bn
conv5_block1_0_relu
conv5_block1_1_conv
conv5_block1_1_bn
conv5_block1_1_relu
conv5_block1_2_conv
conv5_block1_concat
conv5_block2_0_bn
conv5_block2_0_relu
conv5_block2_1_conv
conv5_block2_1_bn
conv5_block2_1_relu
conv5_block2_2_conv
conv5_block2_concat
conv5_block3_0_bn
conv5_block3_0_relu
conv5_block3_1_conv
conv5_block3_1_bn
conv5_block3_1_relu
conv5_block3_2_conv
conv5_block3_concat
conv5_block4_0_bn
conv5_block4_0_relu
conv5_block4_1_conv
conv5_block4_1_bn
conv5_block4_1_relu
conv5_block4_2_conv
conv5_block4_concat
conv5_block5_0_bn
conv5_block5_0_relu
conv5_block5_1_conv
conv5_block5_1_bn
conv5_block5_1_relu
conv5_block5_2_conv
conv5_block5_concat
conv5_block6_0_bn
conv5_block6_0_relu
conv5_block6_1_conv
conv5_block6_1_bn
conv5_block6_1_relu
conv5_block6_2_conv
conv5_block6_concat
conv5_block7_0_bn
conv5_block7_0_relu
conv5_block7_1_conv
conv5_block7_1_bn
conv5_block7_1_relu
conv5_block7_2_conv
conv5_block7_concat
conv5_block8_0_bn
conv5_block8_0_relu
conv5_block8_1_conv
conv5_block8_1_bn
conv5_block8_1_relu
conv5_block8_2_conv
conv5_block8_concat
conv5_block9_0_bn
conv5_block9_0_relu
conv5_block9_1_conv
conv5_block9_1_bn
conv5_block9_1_relu
conv5_block9_2_conv
conv5_block9_concat
conv5_block10_0_bn
conv5_block10_0_relu
conv5_block10_1_conv
conv5_block10_1_bn
conv5_block10_1_relu
conv5_block10_2_conv
conv5_block10_concat
conv5_block11_0_bn
conv5_block11_0_relu
conv5_block11_1_conv
conv5_block11_1_bn
conv5_block11_1_relu
conv5_block11_2_conv
conv5_block11_concat
conv5_block12_0_bn
conv5_block12_0_relu
conv5_block12_1_conv
conv5_block12_1_bn
conv5_block12_1_relu
conv5_block12_2_conv
conv5_block12_concat
conv5_block13_0_bn
conv5_block13_0_relu
conv5_block13_1_conv
conv5_block13_1_bn
conv5_block13_1_relu
conv5_block13_2_conv
conv5_block13_concat
conv5_block14_0_bn
conv5_block14_0_relu
conv5_block14_1_conv
conv5_block14_1_bn
conv5_block14_1_relu
conv5_block14_2_conv
conv5_block14_concat
conv5_block15_0_bn
conv5_block15_0_relu
conv5_block15_1_conv
conv5_block15_1_bn
conv5_block15_1_relu
conv5_block15_2_conv
conv5_block15_concat
conv5_block16_0_bn
conv5_block16_0_relu
conv5_block16_1_conv
conv5_block16_1_bn
conv5_block16_1_relu
conv5_block16_2_conv
conv5_block16_concat
bn
relu
input_1_mirror
zero_padding2d_6
conv1/conv
conv1/bn
conv1/relu
zero_padding2d_7
pool1
conv2_block1_0_bn
conv2_block1_0_relu
conv2_block1_1_conv
conv2_block1_1_bn
conv2_block1_1_relu
conv2_block1_2_conv
conv2_block1_concat
conv2_block2_0_bn
conv2_block2_0_relu
conv2_block2_1_conv
conv2_block2_1_bn
conv2_block2_1_relu
conv2_block2_2_conv
conv2_block2_concat
conv2_block3_0_bn
conv2_block3_0_relu
conv2_block3_1_conv
conv2_block3_1_bn
conv2_block3_1_relu
conv2_block3_2_conv
conv2_block3_concat
conv2_block4_0_bn
conv2_block4_0_relu
conv2_block4_1_conv
conv2_block4_1_bn
conv2_block4_1_relu
conv2_block4_2_conv
conv2_block4_concat
conv2_block5_0_bn
conv2_block5_0_relu
conv2_block5_1_conv
conv2_block5_1_bn
conv2_block5_1_relu
conv2_block5_2_conv
conv2_block5_concat
conv2_block6_0_bn
conv2_block6_0_relu
conv2_block6_1_conv
conv2_block6_1_bn
conv2_block6_1_relu
conv2_block6_2_conv
conv2_block6_concat
pool2_bn
pool2_relu
pool2_conv
pool2_pool
conv3_block1_0_bn
conv3_block1_0_relu
conv3_block1_1_conv
conv3_block1_1_bn
conv3_block1_1_relu
conv3_block1_2_conv
conv3_block1_concat
conv3_block2_0_bn
conv3_block2_0_relu
conv3_block2_1_conv
conv3_block2_1_bn
conv3_block2_1_relu
conv3_block2_2_conv
conv3_block2_concat
conv3_block3_0_bn
conv3_block3_0_relu
conv3_block3_1_conv
conv3_block3_1_bn
conv3_block3_1_relu
conv3_block3_2_conv
conv3_block3_concat
conv3_block4_0_bn
conv3_block4_0_relu
conv3_block4_1_conv
conv3_block4_1_bn
conv3_block4_1_relu
conv3_block4_2_conv
conv3_block4_concat
conv3_block5_0_bn
conv3_block5_0_relu
conv3_block5_1_conv
conv3_block5_1_bn
conv3_block5_1_relu
conv3_block5_2_conv
conv3_block5_concat
conv3_block6_0_bn
conv3_block6_0_relu
conv3_block6_1_conv
conv3_block6_1_bn
conv3_block6_1_relu
conv3_block6_2_conv
conv3_block6_concat
conv3_block7_0_bn
conv3_block7_0_relu
conv3_block7_1_conv
conv3_block7_1_bn
conv3_block7_1_relu
conv3_block7_2_conv
conv3_block7_concat
conv3_block8_0_bn
conv3_block8_0_relu
conv3_block8_1_conv
conv3_block8_1_bn
conv3_block8_1_relu
conv3_block8_2_conv
conv3_block8_concat
conv3_block9_0_bn
conv3_block9_0_relu
conv3_block9_1_conv
conv3_block9_1_bn
conv3_block9_1_relu
conv3_block9_2_conv
conv3_block9_concat
conv3_block10_0_bn
conv3_block10_0_relu
conv3_block10_1_conv
conv3_block10_1_bn
conv3_block10_1_relu
conv3_block10_2_conv
conv3_block10_concat
conv3_block11_0_bn
conv3_block11_0_relu
conv3_block11_1_conv
conv3_block11_1_bn
conv3_block11_1_relu
conv3_block11_2_conv
conv3_block11_concat
conv3_block12_0_bn
conv3_block12_0_relu
conv3_block12_1_conv
conv3_block12_1_bn
conv3_block12_1_relu
conv3_block12_2_conv
conv3_block12_concat
pool3_bn
pool3_relu
pool3_conv
pool3_pool
conv4_block1_0_bn
conv4_block1_0_relu
conv4_block1_1_conv
conv4_block1_1_bn
conv4_block1_1_relu
conv4_block1_2_conv
conv4_block1_concat
conv4_block2_0_bn
conv4_block2_0_relu
conv4_block2_1_conv
conv4_block2_1_bn
conv4_block2_1_relu
conv4_block2_2_conv
conv4_block2_concat
conv4_block3_0_bn
conv4_block3_0_relu
conv4_block3_1_conv
conv4_block3_1_bn
conv4_block3_1_relu
conv4_block3_2_conv
conv4_block3_concat
conv4_block4_0_bn
conv4_block4_0_relu
conv4_block4_1_conv
conv4_block4_1_bn
conv4_block4_1_relu
conv4_block4_2_conv
conv4_block4_concat
conv4_block5_0_bn
conv4_block5_0_relu
conv4_block5_1_conv
conv4_block5_1_bn
conv4_block5_1_relu
conv4_block5_2_conv
conv4_block5_concat
conv4_block6_0_bn
conv4_block6_0_relu
conv4_block6_1_conv
conv4_block6_1_bn
conv4_block6_1_relu
conv4_block6_2_conv
conv4_block6_concat
conv4_block7_0_bn
conv4_block7_0_relu
conv4_block7_1_conv
conv4_block7_1_bn
conv4_block7_1_relu
conv4_block7_2_conv
conv4_block7_concat
conv4_block8_0_bn
conv4_block8_0_relu
conv4_block8_1_conv
conv4_block8_1_bn
conv4_block8_1_relu
conv4_block8_2_conv
conv4_block8_concat
conv4_block9_0_bn
conv4_block9_0_relu
conv4_block9_1_conv
conv4_block9_1_bn
conv4_block9_1_relu
conv4_block9_2_conv
conv4_block9_concat
conv4_block10_0_bn
conv4_block10_0_relu
conv4_block10_1_conv
conv4_block10_1_bn
conv4_block10_1_relu
conv4_block10_2_conv
conv4_block10_concat
conv4_block11_0_bn
conv4_block11_0_relu
conv4_block11_1_conv
conv4_block11_1_bn
conv4_block11_1_relu
conv4_block11_2_conv
conv4_block11_concat
conv4_block12_0_bn
conv4_block12_0_relu
conv4_block12_1_conv
conv4_block12_1_bn
conv4_block12_1_relu
conv4_block12_2_conv
conv4_block12_concat
conv4_block13_0_bn
conv4_block13_0_relu
conv4_block13_1_conv
conv4_block13_1_bn
conv4_block13_1_relu
conv4_block13_2_conv
conv4_block13_concat
conv4_block14_0_bn
conv4_block14_0_relu
conv4_block14_1_conv
conv4_block14_1_bn
conv4_block14_1_relu
conv4_block14_2_conv
conv4_block14_concat
conv4_block15_0_bn
conv4_block15_0_relu
conv4_block15_1_conv
conv4_block15_1_bn
conv4_block15_1_relu
conv4_block15_2_conv
conv4_block15_concat
conv4_block16_0_bn
conv4_block16_0_relu
conv4_block16_1_conv
conv4_block16_1_bn
conv4_block16_1_relu
conv4_block16_2_conv
conv4_block16_concat
conv4_block17_0_bn
conv4_block17_0_relu
conv4_block17_1_conv
conv4_block17_1_bn
conv4_block17_1_relu
conv4_block17_2_conv
conv4_block17_concat
conv4_block18_0_bn
conv4_block18_0_relu
conv4_block18_1_conv
conv4_block18_1_bn
conv4_block18_1_relu
conv4_block18_2_conv
conv4_block18_concat
conv4_block19_0_bn
conv4_block19_0_relu
conv4_block19_1_conv
conv4_block19_1_bn
conv4_block19_1_relu
conv4_block19_2_conv
conv4_block19_concat
conv4_block20_0_bn
conv4_block20_0_relu
conv4_block20_1_conv
conv4_block20_1_bn
conv4_block20_1_relu
conv4_block20_2_conv
conv4_block20_concat
conv4_block21_0_bn
conv4_block21_0_relu
conv4_block21_1_conv
conv4_block21_1_bn
conv4_block21_1_relu
conv4_block21_2_conv
conv4_block21_concat
conv4_block22_0_bn
conv4_block22_0_relu
conv4_block22_1_conv
conv4_block22_1_bn
conv4_block22_1_relu
conv4_block22_2_conv
conv4_block22_concat
conv4_block23_0_bn
conv4_block23_0_relu
conv4_block23_1_conv
conv4_block23_1_bn
conv4_block23_1_relu
conv4_block23_2_conv
conv4_block23_concat
conv4_block24_0_bn
conv4_block24_0_relu
conv4_block24_1_conv
conv4_block24_1_bn
conv4_block24_1_relu
conv4_block24_2_conv
conv4_block24_concat
pool4_bn
pool4_relu
pool4_conv
pool4_pool
conv5_block1_0_bn
conv5_block1_0_relu
conv5_block1_1_conv
conv5_block1_1_bn
conv5_block1_1_relu
conv5_block1_2_conv
conv5_block1_concat
conv5_block2_0_bn
conv5_block2_0_relu
conv5_block2_1_conv
conv5_block2_1_bn
conv5_block2_1_relu
conv5_block2_2_conv
conv5_block2_concat
conv5_block3_0_bn
conv5_block3_0_relu
conv5_block3_1_conv
conv5_block3_1_bn
conv5_block3_1_relu
conv5_block3_2_conv
conv5_block3_concat
conv5_block4_0_bn
conv5_block4_0_relu
conv5_block4_1_conv
conv5_block4_1_bn
conv5_block4_1_relu
conv5_block4_2_conv
conv5_block4_concat
conv5_block5_0_bn
conv5_block5_0_relu
conv5_block5_1_conv
conv5_block5_1_bn
conv5_block5_1_relu
conv5_block5_2_conv
conv5_block5_concat
conv5_block6_0_bn
conv5_block6_0_relu
conv5_block6_1_conv
conv5_block6_1_bn
conv5_block6_1_relu
conv5_block6_2_conv
conv5_block6_concat
conv5_block7_0_bn
conv5_block7_0_relu
conv5_block7_1_conv
conv5_block7_1_bn
conv5_block7_1_relu
conv5_block7_2_conv
conv5_block7_concat
conv5_block8_0_bn
conv5_block8_0_relu
conv5_block8_1_conv
conv5_block8_1_bn
conv5_block8_1_relu
conv5_block8_2_conv
conv5_block8_concat
conv5_block9_0_bn
conv5_block9_0_relu
conv5_block9_1_conv
conv5_block9_1_bn
conv5_block9_1_relu
conv5_block9_2_conv
conv5_block9_concat
conv5_block10_0_bn
conv5_block10_0_relu
conv5_block10_1_conv
conv5_block10_1_bn
conv5_block10_1_relu
conv5_block10_2_conv
conv5_block10_concat
conv5_block11_0_bn
conv5_block11_0_relu
conv5_block11_1_conv
conv5_block11_1_bn
conv5_block11_1_relu
conv5_block11_2_conv
conv5_block11_concat
conv5_block12_0_bn
conv5_block12_0_relu
conv5_block12_1_conv
conv5_block12_1_bn
conv5_block12_1_relu
conv5_block12_2_conv
conv5_block12_concat
conv5_block13_0_bn
conv5_block13_0_relu
conv5_block13_1_conv
conv5_block13_1_bn
conv5_block13_1_relu
conv5_block13_2_conv
conv5_block13_concat
conv5_block14_0_bn
conv5_block14_0_relu
conv5_block14_1_conv
conv5_block14_1_bn
conv5_block14_1_relu
conv5_block14_2_conv
conv5_block14_concat
conv5_block15_0_bn
conv5_block15_0_relu
conv5_block15_1_conv
conv5_block15_1_bn
conv5_block15_1_relu
conv5_block15_2_conv
conv5_block15_concat
conv5_block16_0_bn
conv5_block16_0_relu
conv5_block16_1_conv
conv5_block16_1_bn
conv5_block16_1_relu
conv5_block16_2_conv
conv5_block16_concat
bn
relu
Concatenation success!
Fused-DenseNet-Tiny ready to connect with its ending layers!


PLEASE CHECK THE MODEL UP TO THE END
Fused-DenseNet-Tiny complete and ready for compilation and training!



C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\keras\engine\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
  warnings.warn('`Model.fit_generator` is deprecated and '
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.
  warnings.warn('This ImageDataGenerator specifies '
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.
  warnings.warn('This ImageDataGenerator specifies '
2021-05-17 11:41:23.152446: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/12
2021-05-17 11:41:26.380168: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-17 11:41:27.559469: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-17 11:41:27.792458: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-17 11:41:30.170908: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0

2021-05-17 11:41:30.295825: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0

2021-05-17 11:41:31.010795: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 997.86MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 11:41:31.052464: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 11:41:32.047845: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 11:41:32.073293: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 11:41:32.204217: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 952.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 11:41:32.228731: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 973.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 11:41:32.269602: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 820.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 11:41:32.292627: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 841.09MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
530/747 [====================>.........] - ETA: 1:08 - loss: 0.7847 - accuracy: 0.65082021-05-17 11:44:21.340122: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 976.20MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 11:44:21.373203: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.09GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
747/747 [==============================] - 270s 349ms/step - loss: 0.7115 - accuracy: 0.6875
Epoch 2/12
747/747 [==============================] - 249s 333ms/step - loss: 0.3152 - accuracy: 0.8779
Epoch 3/12
747/747 [==============================] - 210s 281ms/step - loss: 0.2523 - accuracy: 0.9024
Epoch 4/12
747/747 [==============================] - 212s 284ms/step - loss: 0.2071 - accuracy: 0.9311
Epoch 5/12
747/747 [==============================] - 209s 279ms/step - loss: 0.1837 - accuracy: 0.9314
Epoch 6/12
747/747 [==============================] - 209s 280ms/step - loss: 0.1781 - accuracy: 0.9330
Epoch 7/12
747/747 [==============================] - 209s 280ms/step - loss: 0.1745 - accuracy: 0.9424
Epoch 8/12
747/747 [==============================] - 212s 283ms/step - loss: 0.1597 - accuracy: 0.9465
Epoch 9/12
747/747 [==============================] - 210s 281ms/step - loss: 0.1506 - accuracy: 0.9477
Epoch 10/12
747/747 [==============================] - 209s 280ms/step - loss: 0.1505 - accuracy: 0.9465
Epoch 11/12
747/747 [==============================] - 210s 281ms/step - loss: 0.1406 - accuracy: 0.9532
Epoch 12/12
747/747 [==============================] - 211s 282ms/step - loss: 0.1413 - accuracy: 0.9528
------------------------------------------------------------------------
Training for fold4
Found 1493 images belonging to 3 classes.
1493/1493 [==============================] - 41s 26ms/step - loss: 0.9558 - accuracy: 0.7033
Test loss: 0.9557744264602661
Test accuracy: 0.7032819986343384
              precision    recall  f1-score   support

           0       0.98      0.67      0.79       320
           1       0.43      1.00      0.60       318
           2       0.98      0.61      0.75       855

    accuracy                           0.70      1493
   macro avg       0.79      0.76      0.71      1493
weighted avg       0.86      0.70      0.73      1493

------------------------------------------------------------------------
------------------------------------------------------------------------
Score per fold
------------------------------------------------------------------------
> Fold 1 - Loss: 0.9557744264602661 - Accuracy: 0.7032819986343384%
------------------------------------------------------------------------
Average scores for all folds:
> Accuracy: 0.7032819986343384 (+- 0.0)
> Loss: 0.9557744264602661
------------------------------------------------------------------------

Process finished with exit code 0
