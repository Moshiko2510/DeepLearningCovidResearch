C:\Users\Daniel-PC\PycharmProjects\cnn\venv\Scripts\python.exe C:/Users/Daniel-PC/PycharmProjects/cnn/venv/r4_densenet.py
2021-05-17 12:33:45.315908: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:349: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, which overrides setting of `featurewise_center`.
  warnings.warn('This ImageDataGenerator specifies '
3
Found 5962 images belonging to 3 classes.
Found 1511 images belonging to 3 classes.
2021-05-17 12:33:50.934523: I tensorflow/compiler/jit/xla_cpu_device.cc:41] Not creating XLA devices, tf_xla_enable_xla_devices not set
2021-05-17 12:33:50.946662: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library nvcuda.dll
2021-05-17 12:33:51.773957: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2021-05-17 12:33:51.774516: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-05-17 12:33:52.035082: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-17 12:33:52.035333: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-17 12:33:52.187868: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-05-17 12:33:52.213717: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-05-17 12:33:52.412086: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-05-17 12:33:52.522843: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-05-17 12:33:52.536042: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-17 12:33:52.778539: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-05-17 12:33:52.779337: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-05-17 12:33:52.781151: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1720] Found device 0 with properties: 
pciBusID: 0000:02:00.0 name: NVIDIA GeForce MX250 computeCapability: 6.1
coreClock: 1.582GHz coreCount: 3 deviceMemorySize: 2.00GiB deviceMemoryBandwidth: 44.76GiB/s
2021-05-17 12:33:52.782485: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudart64_110.dll
2021-05-17 12:33:52.783093: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-17 12:33:52.783630: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-17 12:33:52.784027: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cufft64_10.dll
2021-05-17 12:33:52.784341: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library curand64_10.dll
2021-05-17 12:33:52.784683: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusolver64_10.dll
2021-05-17 12:33:52.785032: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cusparse64_11.dll
2021-05-17 12:33:52.785381: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-17 12:33:52.785771: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1862] Adding visible gpu devices: 0
2021-05-17 12:33:53.463083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1261] Device interconnect StreamExecutor with strength 1 edge matrix:
2021-05-17 12:33:53.463635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1267]      0 
2021-05-17 12:33:53.463874: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1280] 0:   N 
2021-05-17 12:33:53.464316: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1406] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 1342 MB memory) -> physical GPU (device: 0, name: NVIDIA GeForce MX250, pci bus id: 0000:02:00.0, compute capability: 6.1)
2021-05-17 12:33:53.465938: I tensorflow/compiler/jit/xla_gpu_device.cc:99] Not creating XLA devices, tf_xla_enable_xla_devices not set
input_1
zero_padding2d_2
conv1/conv
conv1/bn
conv1/relu
zero_padding2d_3
pool1
conv2_block1_0_bn
conv2_block1_0_relu
conv2_block1_1_conv
conv2_block1_1_bn
conv2_block1_1_relu
conv2_block1_2_conv
conv2_block1_concat
conv2_block2_0_bn
conv2_block2_0_relu
conv2_block2_1_conv
conv2_block2_1_bn
conv2_block2_1_relu
conv2_block2_2_conv
conv2_block2_concat
conv2_block3_0_bn
conv2_block3_0_relu
conv2_block3_1_conv
conv2_block3_1_bn
conv2_block3_1_relu
conv2_block3_2_conv
conv2_block3_concat
conv2_block4_0_bn
conv2_block4_0_relu
conv2_block4_1_conv
conv2_block4_1_bn
conv2_block4_1_relu
conv2_block4_2_conv
conv2_block4_concat
conv2_block5_0_bn
conv2_block5_0_relu
conv2_block5_1_conv
conv2_block5_1_bn
conv2_block5_1_relu
conv2_block5_2_conv
conv2_block5_concat
conv2_block6_0_bn
conv2_block6_0_relu
conv2_block6_1_conv
conv2_block6_1_bn
conv2_block6_1_relu
conv2_block6_2_conv
conv2_block6_concat
pool2_bn
pool2_relu
pool2_conv
pool2_pool
conv3_block1_0_bn
conv3_block1_0_relu
conv3_block1_1_conv
conv3_block1_1_bn
conv3_block1_1_relu
conv3_block1_2_conv
conv3_block1_concat
conv3_block2_0_bn
conv3_block2_0_relu
conv3_block2_1_conv
conv3_block2_1_bn
conv3_block2_1_relu
conv3_block2_2_conv
conv3_block2_concat
conv3_block3_0_bn
conv3_block3_0_relu
conv3_block3_1_conv
conv3_block3_1_bn
conv3_block3_1_relu
conv3_block3_2_conv
conv3_block3_concat
conv3_block4_0_bn
conv3_block4_0_relu
conv3_block4_1_conv
conv3_block4_1_bn
conv3_block4_1_relu
conv3_block4_2_conv
conv3_block4_concat
conv3_block5_0_bn
conv3_block5_0_relu
conv3_block5_1_conv
conv3_block5_1_bn
conv3_block5_1_relu
conv3_block5_2_conv
conv3_block5_concat
conv3_block6_0_bn
conv3_block6_0_relu
conv3_block6_1_conv
conv3_block6_1_bn
conv3_block6_1_relu
conv3_block6_2_conv
conv3_block6_concat
conv3_block7_0_bn
conv3_block7_0_relu
conv3_block7_1_conv
conv3_block7_1_bn
conv3_block7_1_relu
conv3_block7_2_conv
conv3_block7_concat
conv3_block8_0_bn
conv3_block8_0_relu
conv3_block8_1_conv
conv3_block8_1_bn
conv3_block8_1_relu
conv3_block8_2_conv
conv3_block8_concat
conv3_block9_0_bn
conv3_block9_0_relu
conv3_block9_1_conv
conv3_block9_1_bn
conv3_block9_1_relu
conv3_block9_2_conv
conv3_block9_concat
conv3_block10_0_bn
conv3_block10_0_relu
conv3_block10_1_conv
conv3_block10_1_bn
conv3_block10_1_relu
conv3_block10_2_conv
conv3_block10_concat
conv3_block11_0_bn
conv3_block11_0_relu
conv3_block11_1_conv
conv3_block11_1_bn
conv3_block11_1_relu
conv3_block11_2_conv
conv3_block11_concat
conv3_block12_0_bn
conv3_block12_0_relu
conv3_block12_1_conv
conv3_block12_1_bn
conv3_block12_1_relu
conv3_block12_2_conv
conv3_block12_concat
pool3_bn
pool3_relu
pool3_conv
pool3_pool
conv4_block1_0_bn
conv4_block1_0_relu
conv4_block1_1_conv
conv4_block1_1_bn
conv4_block1_1_relu
conv4_block1_2_conv
conv4_block1_concat
conv4_block2_0_bn
conv4_block2_0_relu
conv4_block2_1_conv
conv4_block2_1_bn
conv4_block2_1_relu
conv4_block2_2_conv
conv4_block2_concat
conv4_block3_0_bn
conv4_block3_0_relu
conv4_block3_1_conv
conv4_block3_1_bn
conv4_block3_1_relu
conv4_block3_2_conv
conv4_block3_concat
conv4_block4_0_bn
conv4_block4_0_relu
conv4_block4_1_conv
conv4_block4_1_bn
conv4_block4_1_relu
conv4_block4_2_conv
conv4_block4_concat
conv4_block5_0_bn
conv4_block5_0_relu
conv4_block5_1_conv
conv4_block5_1_bn
conv4_block5_1_relu
conv4_block5_2_conv
conv4_block5_concat
conv4_block6_0_bn
conv4_block6_0_relu
conv4_block6_1_conv
conv4_block6_1_bn
conv4_block6_1_relu
conv4_block6_2_conv
conv4_block6_concat
conv4_block7_0_bn
conv4_block7_0_relu
conv4_block7_1_conv
conv4_block7_1_bn
conv4_block7_1_relu
conv4_block7_2_conv
conv4_block7_concat
conv4_block8_0_bn
conv4_block8_0_relu
conv4_block8_1_conv
conv4_block8_1_bn
conv4_block8_1_relu
conv4_block8_2_conv
conv4_block8_concat
conv4_block9_0_bn
conv4_block9_0_relu
conv4_block9_1_conv
conv4_block9_1_bn
conv4_block9_1_relu
conv4_block9_2_conv
conv4_block9_concat
conv4_block10_0_bn
conv4_block10_0_relu
conv4_block10_1_conv
conv4_block10_1_bn
conv4_block10_1_relu
conv4_block10_2_conv
conv4_block10_concat
conv4_block11_0_bn
conv4_block11_0_relu
conv4_block11_1_conv
conv4_block11_1_bn
conv4_block11_1_relu
conv4_block11_2_conv
conv4_block11_concat
conv4_block12_0_bn
conv4_block12_0_relu
conv4_block12_1_conv
conv4_block12_1_bn
conv4_block12_1_relu
conv4_block12_2_conv
conv4_block12_concat
conv4_block13_0_bn
conv4_block13_0_relu
conv4_block13_1_conv
conv4_block13_1_bn
conv4_block13_1_relu
conv4_block13_2_conv
conv4_block13_concat
conv4_block14_0_bn
conv4_block14_0_relu
conv4_block14_1_conv
conv4_block14_1_bn
conv4_block14_1_relu
conv4_block14_2_conv
conv4_block14_concat
conv4_block15_0_bn
conv4_block15_0_relu
conv4_block15_1_conv
conv4_block15_1_bn
conv4_block15_1_relu
conv4_block15_2_conv
conv4_block15_concat
conv4_block16_0_bn
conv4_block16_0_relu
conv4_block16_1_conv
conv4_block16_1_bn
conv4_block16_1_relu
conv4_block16_2_conv
conv4_block16_concat
conv4_block17_0_bn
conv4_block17_0_relu
conv4_block17_1_conv
conv4_block17_1_bn
conv4_block17_1_relu
conv4_block17_2_conv
conv4_block17_concat
conv4_block18_0_bn
conv4_block18_0_relu
conv4_block18_1_conv
conv4_block18_1_bn
conv4_block18_1_relu
conv4_block18_2_conv
conv4_block18_concat
conv4_block19_0_bn
conv4_block19_0_relu
conv4_block19_1_conv
conv4_block19_1_bn
conv4_block19_1_relu
conv4_block19_2_conv
conv4_block19_concat
conv4_block20_0_bn
conv4_block20_0_relu
conv4_block20_1_conv
conv4_block20_1_bn
conv4_block20_1_relu
conv4_block20_2_conv
conv4_block20_concat
conv4_block21_0_bn
conv4_block21_0_relu
conv4_block21_1_conv
conv4_block21_1_bn
conv4_block21_1_relu
conv4_block21_2_conv
conv4_block21_concat
conv4_block22_0_bn
conv4_block22_0_relu
conv4_block22_1_conv
conv4_block22_1_bn
conv4_block22_1_relu
conv4_block22_2_conv
conv4_block22_concat
conv4_block23_0_bn
conv4_block23_0_relu
conv4_block23_1_conv
conv4_block23_1_bn
conv4_block23_1_relu
conv4_block23_2_conv
conv4_block23_concat
conv4_block24_0_bn
conv4_block24_0_relu
conv4_block24_1_conv
conv4_block24_1_bn
conv4_block24_1_relu
conv4_block24_2_conv
conv4_block24_concat
pool4_bn
pool4_relu
pool4_conv
pool4_pool
conv5_block1_0_bn
conv5_block1_0_relu
conv5_block1_1_conv
conv5_block1_1_bn
conv5_block1_1_relu
conv5_block1_2_conv
conv5_block1_concat
conv5_block2_0_bn
conv5_block2_0_relu
conv5_block2_1_conv
conv5_block2_1_bn
conv5_block2_1_relu
conv5_block2_2_conv
conv5_block2_concat
conv5_block3_0_bn
conv5_block3_0_relu
conv5_block3_1_conv
conv5_block3_1_bn
conv5_block3_1_relu
conv5_block3_2_conv
conv5_block3_concat
conv5_block4_0_bn
conv5_block4_0_relu
conv5_block4_1_conv
conv5_block4_1_bn
conv5_block4_1_relu
conv5_block4_2_conv
conv5_block4_concat
conv5_block5_0_bn
conv5_block5_0_relu
conv5_block5_1_conv
conv5_block5_1_bn
conv5_block5_1_relu
conv5_block5_2_conv
conv5_block5_concat
conv5_block6_0_bn
conv5_block6_0_relu
conv5_block6_1_conv
conv5_block6_1_bn
conv5_block6_1_relu
conv5_block6_2_conv
conv5_block6_concat
conv5_block7_0_bn
conv5_block7_0_relu
conv5_block7_1_conv
conv5_block7_1_bn
conv5_block7_1_relu
conv5_block7_2_conv
conv5_block7_concat
conv5_block8_0_bn
conv5_block8_0_relu
conv5_block8_1_conv
conv5_block8_1_bn
conv5_block8_1_relu
conv5_block8_2_conv
conv5_block8_concat
conv5_block9_0_bn
conv5_block9_0_relu
conv5_block9_1_conv
conv5_block9_1_bn
conv5_block9_1_relu
conv5_block9_2_conv
conv5_block9_concat
conv5_block10_0_bn
conv5_block10_0_relu
conv5_block10_1_conv
conv5_block10_1_bn
conv5_block10_1_relu
conv5_block10_2_conv
conv5_block10_concat
conv5_block11_0_bn
conv5_block11_0_relu
conv5_block11_1_conv
conv5_block11_1_bn
conv5_block11_1_relu
conv5_block11_2_conv
conv5_block11_concat
conv5_block12_0_bn
conv5_block12_0_relu
conv5_block12_1_conv
conv5_block12_1_bn
conv5_block12_1_relu
conv5_block12_2_conv
conv5_block12_concat
conv5_block13_0_bn
conv5_block13_0_relu
conv5_block13_1_conv
conv5_block13_1_bn
conv5_block13_1_relu
conv5_block13_2_conv
conv5_block13_concat
conv5_block14_0_bn
conv5_block14_0_relu
conv5_block14_1_conv
conv5_block14_1_bn
conv5_block14_1_relu
conv5_block14_2_conv
conv5_block14_concat
conv5_block15_0_bn
conv5_block15_0_relu
conv5_block15_1_conv
conv5_block15_1_bn
conv5_block15_1_relu
conv5_block15_2_conv
conv5_block15_concat
conv5_block16_0_bn
conv5_block16_0_relu
conv5_block16_1_conv
conv5_block16_1_bn
conv5_block16_1_relu
conv5_block16_2_conv
conv5_block16_concat
bn
relu
input_1_mirror
zero_padding2d_6
conv1/conv
conv1/bn
conv1/relu
zero_padding2d_7
pool1
conv2_block1_0_bn
conv2_block1_0_relu
conv2_block1_1_conv
conv2_block1_1_bn
conv2_block1_1_relu
conv2_block1_2_conv
conv2_block1_concat
conv2_block2_0_bn
conv2_block2_0_relu
conv2_block2_1_conv
conv2_block2_1_bn
conv2_block2_1_relu
conv2_block2_2_conv
conv2_block2_concat
conv2_block3_0_bn
conv2_block3_0_relu
conv2_block3_1_conv
conv2_block3_1_bn
conv2_block3_1_relu
conv2_block3_2_conv
conv2_block3_concat
conv2_block4_0_bn
conv2_block4_0_relu
conv2_block4_1_conv
conv2_block4_1_bn
conv2_block4_1_relu
conv2_block4_2_conv
conv2_block4_concat
conv2_block5_0_bn
conv2_block5_0_relu
conv2_block5_1_conv
conv2_block5_1_bn
conv2_block5_1_relu
conv2_block5_2_conv
conv2_block5_concat
conv2_block6_0_bn
conv2_block6_0_relu
conv2_block6_1_conv
conv2_block6_1_bn
conv2_block6_1_relu
conv2_block6_2_conv
conv2_block6_concat
pool2_bn
pool2_relu
pool2_conv
pool2_pool
conv3_block1_0_bn
conv3_block1_0_relu
conv3_block1_1_conv
conv3_block1_1_bn
conv3_block1_1_relu
conv3_block1_2_conv
conv3_block1_concat
conv3_block2_0_bn
conv3_block2_0_relu
conv3_block2_1_conv
conv3_block2_1_bn
conv3_block2_1_relu
conv3_block2_2_conv
conv3_block2_concat
conv3_block3_0_bn
conv3_block3_0_relu
conv3_block3_1_conv
conv3_block3_1_bn
conv3_block3_1_relu
conv3_block3_2_conv
conv3_block3_concat
conv3_block4_0_bn
conv3_block4_0_relu
conv3_block4_1_conv
conv3_block4_1_bn
conv3_block4_1_relu
conv3_block4_2_conv
conv3_block4_concat
conv3_block5_0_bn
conv3_block5_0_relu
conv3_block5_1_conv
conv3_block5_1_bn
conv3_block5_1_relu
conv3_block5_2_conv
conv3_block5_concat
conv3_block6_0_bn
conv3_block6_0_relu
conv3_block6_1_conv
conv3_block6_1_bn
conv3_block6_1_relu
conv3_block6_2_conv
conv3_block6_concat
conv3_block7_0_bn
conv3_block7_0_relu
conv3_block7_1_conv
conv3_block7_1_bn
conv3_block7_1_relu
conv3_block7_2_conv
conv3_block7_concat
conv3_block8_0_bn
conv3_block8_0_relu
conv3_block8_1_conv
conv3_block8_1_bn
conv3_block8_1_relu
conv3_block8_2_conv
conv3_block8_concat
conv3_block9_0_bn
conv3_block9_0_relu
conv3_block9_1_conv
conv3_block9_1_bn
conv3_block9_1_relu
conv3_block9_2_conv
conv3_block9_concat
conv3_block10_0_bn
conv3_block10_0_relu
conv3_block10_1_conv
conv3_block10_1_bn
conv3_block10_1_relu
conv3_block10_2_conv
conv3_block10_concat
conv3_block11_0_bn
conv3_block11_0_relu
conv3_block11_1_conv
conv3_block11_1_bn
conv3_block11_1_relu
conv3_block11_2_conv
conv3_block11_concat
conv3_block12_0_bn
conv3_block12_0_relu
conv3_block12_1_conv
conv3_block12_1_bn
conv3_block12_1_relu
conv3_block12_2_conv
conv3_block12_concat
pool3_bn
pool3_relu
pool3_conv
pool3_pool
conv4_block1_0_bn
conv4_block1_0_relu
conv4_block1_1_conv
conv4_block1_1_bn
conv4_block1_1_relu
conv4_block1_2_conv
conv4_block1_concat
conv4_block2_0_bn
conv4_block2_0_relu
conv4_block2_1_conv
conv4_block2_1_bn
conv4_block2_1_relu
conv4_block2_2_conv
conv4_block2_concat
conv4_block3_0_bn
conv4_block3_0_relu
conv4_block3_1_conv
conv4_block3_1_bn
conv4_block3_1_relu
conv4_block3_2_conv
conv4_block3_concat
conv4_block4_0_bn
conv4_block4_0_relu
conv4_block4_1_conv
conv4_block4_1_bn
conv4_block4_1_relu
conv4_block4_2_conv
conv4_block4_concat
conv4_block5_0_bn
conv4_block5_0_relu
conv4_block5_1_conv
conv4_block5_1_bn
conv4_block5_1_relu
conv4_block5_2_conv
conv4_block5_concat
conv4_block6_0_bn
conv4_block6_0_relu
conv4_block6_1_conv
conv4_block6_1_bn
conv4_block6_1_relu
conv4_block6_2_conv
conv4_block6_concat
conv4_block7_0_bn
conv4_block7_0_relu
conv4_block7_1_conv
conv4_block7_1_bn
conv4_block7_1_relu
conv4_block7_2_conv
conv4_block7_concat
conv4_block8_0_bn
conv4_block8_0_relu
conv4_block8_1_conv
conv4_block8_1_bn
conv4_block8_1_relu
conv4_block8_2_conv
conv4_block8_concat
conv4_block9_0_bn
conv4_block9_0_relu
conv4_block9_1_conv
conv4_block9_1_bn
conv4_block9_1_relu
conv4_block9_2_conv
conv4_block9_concat
conv4_block10_0_bn
conv4_block10_0_relu
conv4_block10_1_conv
conv4_block10_1_bn
conv4_block10_1_relu
conv4_block10_2_conv
conv4_block10_concat
conv4_block11_0_bn
conv4_block11_0_relu
conv4_block11_1_conv
conv4_block11_1_bn
conv4_block11_1_relu
conv4_block11_2_conv
conv4_block11_concat
conv4_block12_0_bn
conv4_block12_0_relu
conv4_block12_1_conv
conv4_block12_1_bn
conv4_block12_1_relu
conv4_block12_2_conv
conv4_block12_concat
conv4_block13_0_bn
conv4_block13_0_relu
conv4_block13_1_conv
conv4_block13_1_bn
conv4_block13_1_relu
conv4_block13_2_conv
conv4_block13_concat
conv4_block14_0_bn
conv4_block14_0_relu
conv4_block14_1_conv
conv4_block14_1_bn
conv4_block14_1_relu
conv4_block14_2_conv
conv4_block14_concat
conv4_block15_0_bn
conv4_block15_0_relu
conv4_block15_1_conv
conv4_block15_1_bn
conv4_block15_1_relu
conv4_block15_2_conv
conv4_block15_concat
conv4_block16_0_bn
conv4_block16_0_relu
conv4_block16_1_conv
conv4_block16_1_bn
conv4_block16_1_relu
conv4_block16_2_conv
conv4_block16_concat
conv4_block17_0_bn
conv4_block17_0_relu
conv4_block17_1_conv
conv4_block17_1_bn
conv4_block17_1_relu
conv4_block17_2_conv
conv4_block17_concat
conv4_block18_0_bn
conv4_block18_0_relu
conv4_block18_1_conv
conv4_block18_1_bn
conv4_block18_1_relu
conv4_block18_2_conv
conv4_block18_concat
conv4_block19_0_bn
conv4_block19_0_relu
conv4_block19_1_conv
conv4_block19_1_bn
conv4_block19_1_relu
conv4_block19_2_conv
conv4_block19_concat
conv4_block20_0_bn
conv4_block20_0_relu
conv4_block20_1_conv
conv4_block20_1_bn
conv4_block20_1_relu
conv4_block20_2_conv
conv4_block20_concat
conv4_block21_0_bn
conv4_block21_0_relu
conv4_block21_1_conv
conv4_block21_1_bn
conv4_block21_1_relu
conv4_block21_2_conv
conv4_block21_concat
conv4_block22_0_bn
conv4_block22_0_relu
conv4_block22_1_conv
conv4_block22_1_bn
conv4_block22_1_relu
conv4_block22_2_conv
conv4_block22_concat
conv4_block23_0_bn
conv4_block23_0_relu
conv4_block23_1_conv
conv4_block23_1_bn
conv4_block23_1_relu
conv4_block23_2_conv
conv4_block23_concat
conv4_block24_0_bn
conv4_block24_0_relu
conv4_block24_1_conv
conv4_block24_1_bn
conv4_block24_1_relu
conv4_block24_2_conv
conv4_block24_concat
pool4_bn
pool4_relu
pool4_conv
pool4_pool
conv5_block1_0_bn
conv5_block1_0_relu
conv5_block1_1_conv
conv5_block1_1_bn
conv5_block1_1_relu
conv5_block1_2_conv
conv5_block1_concat
conv5_block2_0_bn
conv5_block2_0_relu
conv5_block2_1_conv
conv5_block2_1_bn
conv5_block2_1_relu
conv5_block2_2_conv
conv5_block2_concat
conv5_block3_0_bn
conv5_block3_0_relu
conv5_block3_1_conv
conv5_block3_1_bn
conv5_block3_1_relu
conv5_block3_2_conv
conv5_block3_concat
conv5_block4_0_bn
conv5_block4_0_relu
conv5_block4_1_conv
conv5_block4_1_bn
conv5_block4_1_relu
conv5_block4_2_conv
conv5_block4_concat
conv5_block5_0_bn
conv5_block5_0_relu
conv5_block5_1_conv
conv5_block5_1_bn
conv5_block5_1_relu
conv5_block5_2_conv
conv5_block5_concat
conv5_block6_0_bn
conv5_block6_0_relu
conv5_block6_1_conv
conv5_block6_1_bn
conv5_block6_1_relu
conv5_block6_2_conv
conv5_block6_concat
conv5_block7_0_bn
conv5_block7_0_relu
conv5_block7_1_conv
conv5_block7_1_bn
conv5_block7_1_relu
conv5_block7_2_conv
conv5_block7_concat
conv5_block8_0_bn
conv5_block8_0_relu
conv5_block8_1_conv
conv5_block8_1_bn
conv5_block8_1_relu
conv5_block8_2_conv
conv5_block8_concat
conv5_block9_0_bn
conv5_block9_0_relu
conv5_block9_1_conv
conv5_block9_1_bn
conv5_block9_1_relu
conv5_block9_2_conv
conv5_block9_concat
conv5_block10_0_bn
conv5_block10_0_relu
conv5_block10_1_conv
conv5_block10_1_bn
conv5_block10_1_relu
conv5_block10_2_conv
conv5_block10_concat
conv5_block11_0_bn
conv5_block11_0_relu
conv5_block11_1_conv
conv5_block11_1_bn
conv5_block11_1_relu
conv5_block11_2_conv
conv5_block11_concat
conv5_block12_0_bn
conv5_block12_0_relu
conv5_block12_1_conv
conv5_block12_1_bn
conv5_block12_1_relu
conv5_block12_2_conv
conv5_block12_concat
conv5_block13_0_bn
conv5_block13_0_relu
conv5_block13_1_conv
conv5_block13_1_bn
conv5_block13_1_relu
conv5_block13_2_conv
conv5_block13_concat
conv5_block14_0_bn
conv5_block14_0_relu
conv5_block14_1_conv
conv5_block14_1_bn
conv5_block14_1_relu
conv5_block14_2_conv
conv5_block14_concat
conv5_block15_0_bn
conv5_block15_0_relu
conv5_block15_1_conv
conv5_block15_1_bn
conv5_block15_1_relu
conv5_block15_2_conv
conv5_block15_concat
conv5_block16_0_bn
conv5_block16_0_relu
conv5_block16_1_conv
conv5_block16_1_bn
conv5_block16_1_relu
conv5_block16_2_conv
conv5_block16_concat
bn
relu
Concatenation success!
Fused-DenseNet-Tiny ready to connect with its ending layers!


PLEASE CHECK THE MODEL UP TO THE END
Fused-DenseNet-Tiny complete and ready for compilation and training!



C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\tensorflow\python\keras\engine\training.py:1844: UserWarning: `Model.fit_generator` is deprecated and will be removed in a future version. Please use `Model.fit`, which supports generators.
  warnings.warn('`Model.fit_generator` is deprecated and '
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:720: UserWarning: This ImageDataGenerator specifies `featurewise_center`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.
  warnings.warn('This ImageDataGenerator specifies '
C:\Users\Daniel-PC\PycharmProjects\cnn\venv\lib\site-packages\keras_preprocessing\image\image_data_generator.py:728: UserWarning: This ImageDataGenerator specifies `featurewise_std_normalization`, but it hasn't been fit on any training data. Fit it first by calling `.fit(numpy_data)`.
  warnings.warn('This ImageDataGenerator specifies '
2021-05-17 12:34:02.459269: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:116] None of the MLIR optimization passes are enabled (registered 2)
Epoch 1/12
2021-05-17 12:34:04.559464: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublas64_11.dll
2021-05-17 12:34:04.922233: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cublasLt64_11.dll
2021-05-17 12:34:05.076420: I tensorflow/stream_executor/platform/default/dso_loader.cc:49] Successfully opened dynamic library cudnn64_8.dll
2021-05-17 12:34:06.437635: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0

2021-05-17 12:34:06.494308: I tensorflow/core/platform/windows/subprocess.cc:308] SubProcess ended with return code: 0

2021-05-17 12:34:07.043918: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 997.86MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 12:34:07.085376: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.11GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 12:34:07.595314: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.06GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 12:34:07.619949: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.08GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 12:34:07.749045: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 952.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 12:34:07.773004: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 973.11MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 12:34:07.813000: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 820.25MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 12:34:07.834247: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 841.09MiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
173/745 [=====>........................] - ETA: 2:34 - loss: 0.9209 - accuracy: 0.58842021-05-17 12:34:55.666978: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
2021-05-17 12:34:55.682624: W tensorflow/core/common_runtime/bfc_allocator.cc:248] Allocator (GPU_0_bfc) ran out of memory trying to allocate 1.05GiB with freed_by_count=0. The caller indicates that this is not a failure, but may mean that there could be performance gains if more memory were available.
745/745 [==============================] - 217s 284ms/step - loss: 0.6966 - accuracy: 0.6989
Epoch 2/12
745/745 [==============================] - 218s 292ms/step - loss: 0.3141 - accuracy: 0.8777
Epoch 3/12
745/745 [==============================] - 212s 284ms/step - loss: 0.2600 - accuracy: 0.9046
Epoch 4/12
745/745 [==============================] - 227s 304ms/step - loss: 0.2397 - accuracy: 0.9115
Epoch 5/12
745/745 [==============================] - 209s 281ms/step - loss: 0.2103 - accuracy: 0.9214
Epoch 6/12
745/745 [==============================] - 210s 282ms/step - loss: 0.2042 - accuracy: 0.9261
Epoch 7/12
745/745 [==============================] - 209s 281ms/step - loss: 0.1785 - accuracy: 0.9371
Epoch 8/12
745/745 [==============================] - 209s 280ms/step - loss: 0.1968 - accuracy: 0.9308
Epoch 9/12
745/745 [==============================] - 209s 280ms/step - loss: 0.1625 - accuracy: 0.9389
Epoch 10/12
745/745 [==============================] - 209s 280ms/step - loss: 0.1561 - accuracy: 0.9413
Epoch 11/12
745/745 [==============================] - 208s 279ms/step - loss: 0.1631 - accuracy: 0.9418
Epoch 12/12
745/745 [==============================] - 208s 279ms/step - loss: 0.1535 - accuracy: 0.9433
------------------------------------------------------------------------
Training for fold5
Found 1511 images belonging to 3 classes.
1511/1511 [==============================] - 40s 26ms/step - loss: 0.3426 - accuracy: 0.8729
Test loss: 0.34262144565582275
Test accuracy: 0.8729318380355835
              precision    recall  f1-score   support

           0       1.00      0.88      0.93       339
           1       0.64      1.00      0.78       319
           2       0.99      0.82      0.90       853

    accuracy                           0.87      1511
   macro avg       0.87      0.90      0.87      1511
weighted avg       0.92      0.87      0.88      1511

------------------------------------------------------------------------
------------------------------------------------------------------------
Score per fold
------------------------------------------------------------------------
> Fold 1 - Loss: 0.34262144565582275 - Accuracy: 0.8729318380355835%
------------------------------------------------------------------------
Average scores for all folds:
> Accuracy: 0.8729318380355835 (+- 0.0)
> Loss: 0.34262144565582275
------------------------------------------------------------------------

Process finished with exit code 0
